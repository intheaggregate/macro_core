{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value_iteration"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    value_iteration(\n",
    "        return_function,  # Return for transitioning from current to next state\n",
    "        state_space,      # Vector of possible states\n",
    "        discount_factor;  # Discount factor β ∈ (0,1)\n",
    "        tolerance=1e-6,\n",
    "        max_iterations=1000,\n",
    "        initial_value_function=Dict(s => 0.0 for s in state_space),\n",
    "        indicator=false\n",
    "    )\n",
    "\n",
    "Performs value function iteration for a problem where agent directly chooses next state.\n",
    "\n",
    "Parameters:\n",
    "- return_function: Function(current_state, next_state) -> reward\n",
    "- state_space: Vector of possible states\n",
    "- discount_factor: Discount factor β ∈ (0,1)\n",
    "- tolerance: Convergence tolerance\n",
    "- max_iterations: Maximum number of iterations\n",
    "- initial_value_function: Initial value function\n",
    "- capture: capture the value function and policy at certain iterations\n",
    "\n",
    "Returns:\n",
    "- value_function: Dictionary mapping states to values\n",
    "- policy: Dictionary mapping states to optimal next states\n",
    "- iterations: Number of iterations until convergence\n",
    "- diffs: Array of iteration-wise maximum values of change in value function\n",
    "- pdiffs: Array of iteration-wise maximum values of change in policy function\n",
    "\"\"\"\n",
    "function value_iteration(\n",
    "    return_function,\n",
    "    state_space,\n",
    "    discount_factor;\n",
    "    tolerance=1e-6,\n",
    "    max_iterations=1000,\n",
    "    initial_value_function=Dict(s => 0.0 for s in state_space),\n",
    "    indicator=false\n",
    ")\n",
    "    \n",
    "    diffs = Float64[] # track differences\n",
    "    pdiffs = Float64[] # track differences in policy\n",
    "    captures = []\n",
    "\n",
    "    # Initialize value function\n",
    "    value_function = initial_value_function\n",
    "    policy = Dict(s => s for s in state_space)  # Initial policy: stay at current state\n",
    "    \n",
    "    for iter in 1:max_iterations\n",
    "        max_diff = 0.0\n",
    "        max_policy_diff = 0.0\n",
    "        value_function_new = Dict()\n",
    "        oldpolicy = copy(policy)\n",
    "        \n",
    "        # Update value for each current state\n",
    "        for current_state in state_space\n",
    "            # Find maximum value over all possible next states\n",
    "            values = Float64[]\n",
    "            \n",
    "            # Try each possible next state\n",
    "            for next_state in state_space\n",
    "                # Calculate value of choosing this next state:\n",
    "                # Current reward + discounted future value\n",
    "                value = return_function(current_state, next_state) + \n",
    "                       discount_factor * value_function[next_state]\n",
    "                push!(values, value)\n",
    "            end\n",
    "            \n",
    "            # Update value function and policy\n",
    "            max_value, max_index = findmax(values)\n",
    "            value_function_new[current_state] = max_value\n",
    "            policy[current_state] = state_space[max_index]\n",
    "            \n",
    "            # Track maximum change\n",
    "            max_diff = max(max_diff, abs(value_function_new[current_state] - \n",
    "                                       value_function[current_state]))\n",
    "            max_policy_diff = max(max_policy_diff, abs(log(policy[current_state]) - log(oldpolicy[current_state])))\n",
    "        end\n",
    "\n",
    "        push!(diffs, max_diff)\n",
    "        push!(pdiffs, max_policy_diff)\n",
    "        if indicator\n",
    "            if iter in [100, 200, 300, 400]\n",
    "                push!(captures, (iter, copy(value_function_new)))\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Check for convergence\n",
    "        if max_diff < tolerance\n",
    "            return value_function_new, policy, iter, diffs, pdiffs, captures\n",
    "        end\n",
    "        \n",
    "        value_function = value_function_new\n",
    "    end\n",
    "    \n",
    "    @warn \"Maximum iterations reached without convergence\"\n",
    "    return value_function, policy, max_iterations, diffs, captures\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rck (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function rck(k, k_next)\n",
    "    # production function: f(k) = Ak^α\n",
    "    α = 0.33\n",
    "    A = 1\n",
    "    δ = 0.1\n",
    "    \n",
    "    # Current production\n",
    "    production = A*k^α\n",
    "    \n",
    "    # Investment needed to reach k_next\n",
    "    investment = k_next - (1-δ)*k  # δ is depreciation rate\n",
    "    \n",
    "    # Consumption is production minus investment\n",
    "    consumption = production - investment\n",
    "    \n",
    "    # Return negative infinity for infeasible transitions\n",
    "    if consumption <= 0\n",
    "        return -Inf\n",
    "    end\n",
    "    \n",
    "    # Utility from consumption\n",
    "    return log(consumption)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dict{Any, Any}(0.78253268015094 => 0.09049204883305673, 3.49313402934195 => 3.7468553813326526, 2.8969607120712 => 3.1763820709244457, 0.8222775679689901 => 0.18327451954830976, 1.02100200705924 => 0.607950330544671, 1.35485906473086 => 1.2136684452684265, 2.3723281928729403 => 2.6078345023124125, 1.7364099877841401 => 1.797039725672825, 0.57585926349708 => -0.45079843129220115, 1.6012773692027702 => 1.6008670568205297…), Dict(0.78253268015094 => 1.0845938275681202, 3.49313402934195 => 3.50108300690556, 2.8969607120712 => 2.98439946527091, 0.8222775679689901 => 1.1243387153861701, 1.02100200705924 => 1.3071651993492002, 1.35485906473086 => 1.61717532432999, 2.3723281928729403 => 2.53130774414514, 1.7364099877841401 => 1.96693033712883, 0.57585926349708 => 0.8779204109142601, 1.6012773692027702 => 1.8476956736746801…), 270, [1.4685522456868598, 0.478922162669023, 0.2834249170819234, 0.2244711050115944, 0.17422995916456463, 0.13334601393715362, 0.10059033757508207, 0.07468960152708726, 0.05405385027559606, 0.0469500074548832  …  1.4273041895052074e-6, 1.3702120220671077e-6, 1.315403541202187e-6, 1.262787399536336e-6, 1.2122759032173747e-6, 1.1637848671419704e-6, 1.1172334724030009e-6, 1.0725441335068808e-6, 1.0296423682554234e-6, 9.884566738449507e-7], [2.302585092994046, 1.768149603588921, 0.24610925027944808, 0.10424678977073187, 0.05814908648689787, 0.04002312741651097, 0.03001892998988759, 0.0223611801581573, 0.022872661665991445, 0.0158175529797087  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Any[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up and solve the problem\n",
    "k_min_1, k_max_1 = (0.1 * 3.53287891716), 3.53287891716\n",
    "state_space_1 = range(k_min_1, k_max_1, length=401)\n",
    "discount_factor = 0.96\n",
    "\n",
    "# Solve the problem\n",
    "value_function_1, policy_1, iterations_1, diffs_1, pdiffs_1, captures_dump = value_iteration(\n",
    "    rck,\n",
    "    state_space_1,\n",
    "    discount_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dict{Any, Any}(5.9174855910006 => 5.5710970111738245, 3.9905307030486 => 4.175993876208065, 5.5561815495096 => 5.334936846120469, 5.459833805112 => 5.2703508674281645, 4.5686171694342 => 4.63355101024412, 4.8094865304282 => 4.813065430675243, 5.4116599329132 => 5.237764856458238, 4.3036608723408 => 4.428802117302365, 3.8219221503528003 => 4.034497717666469, 4.0627915113468 => 4.235456621547348…), Dict(5.9174855910006 => 5.5561815495096, 3.9905307030486 => 3.9182698947504, 5.5561815495096 => 5.2671383163168, 5.459833805112 => 5.1707905719192, 4.5686171694342 => 4.4240955528378, 4.8094865304282 => 4.616791041633, 5.4116599329132 => 5.1226166997204, 4.3036608723408 => 4.1832261918438, 3.8219221503528003 => 3.773748278154, 4.0627915113468 => 3.9905307030486…), 294, [1.2850954187010066, 0.4359280135367336, 0.2801741905818871, 0.21440319291668164, 0.17873203380875857, 0.156297713678768, 0.14078971975398957, 0.12914413251501422, 0.1199483531712362, 0.11240804956706762  …  1.3960216387332025e-6, 1.3401807734680915e-6, 1.286573542813585e-6, 1.2351106013497315e-6, 1.1857061767983623e-6, 1.1382779296909007e-6, 1.0927468130361717e-6, 1.0490369408699962e-6, 1.0070754630220335e-6, 9.667924443235165e-7], [0.5198607377121882, 0.2882409991719901, 0.07859812152600298, 0.03762104937363997, 0.022814123299232714, 0.014073375568627533, 0.010169303761567994, 0.009538780732617491, 0.006486285142278447, 0.00644448423433075  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Any[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up and solve the problem\n",
    "k_min_2, k_max_2 = 3.53287891716, 5.9415725271\n",
    "state_space_2 = range(k_min_2, k_max_2, length=101)\n",
    "discount_factor = 0.96\n",
    "\n",
    "# Solve the problem\n",
    "value_function_2, policy_2, iterations_2, diffs_2, pdiffs_2, captures_dump = value_iteration(\n",
    "    rck,\n",
    "    state_space_2,\n",
    "    discount_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n",
      "294"
     ]
    }
   ],
   "source": [
    "print(iterations_1, \"\\n\", iterations_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `plot` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `plot` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/UChicago/Year_3/macro_core/uchicago_macro_core/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W5sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "plot(diffs_1[1:iterations_1], label=\"Γ_1\", title=\"Convergence Rates for Γ_1 and Γ_2\", xlabel=\"Iterations\", ylabel=\"Maximum of Value Function Difference\")\n",
    "plot!(diffs_2[1:iterations_2], label=\"Γ_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `plot` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `plot` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/UChicago/Year_3/macro_core/uchicago_macro_core/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W6sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "plot(pdiffs_1[1:iterations_1], label=\"Γ_1\", title=\"Convergence Rates for Γ_1 and Γ_2\", xlabel=\"Iterations\", ylabel=\"Maximum of Log of Policy Function Difference\")\n",
    "plot!(pdiffs_2[1:iterations_2], label=\"Γ_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Float64, Float64} with 101 entries:\n",
       "  5.91749 => 0.00781546\n",
       "  3.99053 => 0.00688917\n",
       "  5.55618 => 0.00778555\n",
       "  5.45983 => 0.0077682\n",
       "  4.56862 => 0.0073903\n",
       "  4.80949 => 0.00753465\n",
       "  5.41166 => 0.00775796\n",
       "  4.30366 => 0.0071897\n",
       "  3.82192 => 0.00669545\n",
       "  4.06279 => 0.0069651\n",
       "  3.96644 => 0.00686293\n",
       "  4.47227 => 0.00732266\n",
       "  5.8934  => 0.00781512\n",
       "  4.83357 => 0.00754723\n",
       "  5.07444 => 0.00765564\n",
       "  5.53209 => 0.00778159\n",
       "  4.08688 => 0.0069895\n",
       "  4.0387  => 0.00694024\n",
       "  4.35183 => 0.00722964\n",
       "  ⋮       => ⋮"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_1 = Dict(s => (((1-0.96)/0.96) * log(s^(0.33) -0.1s)) for s in state_space_1)\n",
    "w_2 = Dict(s => (((1-0.96)/0.96) * log(s^(0.33) -0.1s)) for s in state_space_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the problem\n",
    "value_function_3, policy_3, iterations_3, diffs_3, pdiffs_3, captures_dump = value_iteration(\n",
    "    rck,\n",
    "    state_space_1,\n",
    "    discount_factor,\n",
    "    initial_value_function=w_1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the problem\n",
    "value_function_4, policy_4, iterations_4, diffs_4, pdiffs_4, captures_dump = value_iteration(\n",
    "    rck,\n",
    "    state_space_2,\n",
    "    discount_factor,\n",
    "    initial_value_function=w_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iterations_3, \"\\n\", iterations_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(diffs_3[1:iterations_3], label=\"Γ_1\", title=\"Convergence Rates for Γ_1 and Γ_2\", xlabel=\"Iterations\", ylabel=\"Maximum of Value Function Difference\")\n",
    "plot!(diffs_4[1:iterations_4], label=\"Γ_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(pdiffs_3[1:iterations_3], label=\"Γ_1\", title=\"Convergence Rates for Γ_1 and Γ_2\", xlabel=\"Iterations\", ylabel=\"Maximum of Log of Policy Function Difference\")\n",
    "plot!(pdiffs_4[1:iterations_4], label=\"Γ_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_3 = Dict(s => value_function_1[3.53287891716] for s in state_space_1)\n",
    "w_4 = Dict(s => (((1-0.96)/0.96) * log(s^(0.33) -0.1s) + (s-3.53287891716)) for s in state_space_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the problem\n",
    "value_function_5, policy_5, iterations_5, diffs_5, pdiffs_5, captures_5 = value_iteration(\n",
    "    rck,\n",
    "    state_space_1,\n",
    "    discount_factor,\n",
    "    initial_value_function=w_3,\n",
    "    indicator=true\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the problem\n",
    "value_function_6, policy_6, iterations_6, diffs_6, pdiffs_6, captures_6 = value_iteration(\n",
    "    rck,\n",
    "    state_space_2,\n",
    "    discount_factor,\n",
    "    initial_value_function=w_4,\n",
    "    indicator=true\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iterations_5, \"\\n\", iterations_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(diffs_5[1:iterations_5], label=\"Γ_1\", title=\"Convergence Rates for Γ_1 and Γ_2\", xlabel=\"Iterations\", ylabel=\"Maximum of Value Function Difference\")\n",
    "plot!(diffs_6[1:iterations_6], label=\"Γ_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(pdiffs_5[1:iterations_5], label=\"Γ_1\", title=\"Convergence Rates for Γ_1 and Γ_2\", xlabel=\"Iterations\", ylabel=\"Maximum of Log of Policy Function Difference\")\n",
    "plot!(pdiffs_6[1:iterations_6], label=\"Γ_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:length(captures_5)\n",
    "    plot(state_space_1, collect(values(captures_5[i][2])), label=\"Γ_1 at Iteration $(captures_5[i][1])\", title=\"Value Function Comparison\", xlabel=\"State\", ylabel=\"Value Function\")\n",
    "    plot!(state_space_2, collect(values(captures_6[i][2])), label=\"Γ_2 at Iteration $(captures_6[i][1])\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(state_space_1, collect(values(captures_5[1][2])), label=\"Γ_1 at Iteration $(captures_5[1][1])\", title=\"Value Function Comparison\", xlabel=\"State\", ylabel=\"Value Function\")\n",
    "plot!(state_space_2, collect(values(captures_6[1][2])), label=\"Γ_2 at Iteration $(captures_6[1][1])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(state_space_1, collect(values(captures_5[2][2])), label=\"Γ_1 at Iteration $(captures_5[2][1])\", title=\"Value Function Comparison\", xlabel=\"State\", ylabel=\"Value Function\")\n",
    "plot!(state_space_2, collect(values(captures_6[2][2])), label=\"Γ_2 at Iteration $(captures_6[2][1])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function value_iteration_skip(\n",
    "    return_function,\n",
    "    state_space,\n",
    "    discount_factor;\n",
    "    tolerance=1e-6,\n",
    "    max_iterations=1000,\n",
    "    initial_value_function=Dict(s => 0.0 for s in state_space),\n",
    "    indicator=false\n",
    ")\n",
    "    \n",
    "    diffs = Float64[] # track differences\n",
    "    pdiffs = Float64[] # track differences in policy\n",
    "    captures = []\n",
    "\n",
    "    # Initialize value function\n",
    "    value_function = initial_value_function\n",
    "    policy = Dict(s => s for s in state_space)  # Initial policy: stay at current state\n",
    "    \n",
    "    for iter in 1:max_iterations\n",
    "        max_diff = 0.0\n",
    "        max_policy_diff = 0.0\n",
    "        value_function_new = Dict()\n",
    "        oldpolicy = copy(policy)\n",
    "        \n",
    "        # Update value for each current state\n",
    "        for current_state in state_space\n",
    "            # Find maximum value over all possible next states\n",
    "            values = Float64[]\n",
    "            \n",
    "            # Try each possible next state\n",
    "            for next_state in state_space\n",
    "                # Calculate value of choosing this next state:\n",
    "                # Current reward + discounted future value\n",
    "                value = return_function(current_state, next_state) + \n",
    "                       discount_factor * value_function[next_state]\n",
    "                push!(values, value)\n",
    "            end\n",
    "            \n",
    "            # Update value function\n",
    "            max_value, max_index = findmax(values)\n",
    "            value_function_new[current_state] = max_value\n",
    "            \n",
    "            # Update policy every 2 iterations\n",
    "            if iter % 2 == 0\n",
    "                policy[current_state] = state_space[max_index]\n",
    "            end\n",
    "            \n",
    "            # Track maximum change\n",
    "            max_diff = max(max_diff, abs(value_function_new[current_state] - \n",
    "                                       value_function[current_state]))\n",
    "            if iter % 2 == 0\n",
    "                max_policy_diff = max(max_policy_diff, abs(log(policy[current_state]) - log(oldpolicy[current_state])))\n",
    "            end\n",
    "        end\n",
    "\n",
    "        push!(diffs, max_diff)\n",
    "        if iter % 2 == 0\n",
    "            push!(pdiffs, max_policy_diff)\n",
    "        end\n",
    "        if indicator\n",
    "            if iter in [100, 200, 300, 400]\n",
    "                push!(captures, (iter, copy(value_function_new)))\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Check for convergence\n",
    "        if max_diff < tolerance\n",
    "            return value_function_new, policy, iter, diffs, pdiffs, captures\n",
    "        end\n",
    "        \n",
    "        value_function = value_function_new\n",
    "    end\n",
    "    \n",
    "    @warn \"Maximum iterations reached without convergence\"\n",
    "    return value_function, policy, max_iterations, diffs, captures\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_function_7, policy_7, iterations_7, diffs_7, pdiffs_7, captures_dump = value_iteration_skip(\n",
    "    rck,\n",
    "    state_space_1,\n",
    "    discount_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_function_8, policy_8, iterations_8, diffs_8, pdiffs_8, captures_dump = value_iteration_skip(\n",
    "    rck,\n",
    "    state_space_2,\n",
    "    discount_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iterations_7, \"\\n\", iterations_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(diffs_7[1:iterations_7], label=\"Γ_1\", title=\"Convergence Rates for Γ_1 and Γ_2\", xlabel=\"Iterations\", ylabel=\"Maximum of Value Function Difference\")\n",
    "plot!(diffs_8[1:iterations_8], label=\"Γ_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(pdiffs_7[1:135], label=\"Γ_1\", title=\"Convergence Rates for Γ_1 and Γ_2\", xlabel=\"Iterations\", ylabel=\"Maximum of Log of Policy Function Difference\")\n",
    "plot!(pdiffs_8[1:147], label=\"Γ_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value_iteration_stochastic"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    value_iteration_stochastic(\n",
    "        return_function,  # Return for transitioning from current to next state\n",
    "        state_space,      # Vector of possible states\n",
    "        shock_space,      # Vector of possible shocks\n",
    "        transition_probabilities,  # Matrix of transition probabilities\n",
    "        discount_factor;  # Discount factor β ∈ (0,1)\n",
    "        tolerance=1e-6,\n",
    "        max_iterations=1000,\n",
    "        initial_value_function=Dict((s, z) => 0.0 for s in state_space for z in shock_space)\n",
    "    )\n",
    "\n",
    "Performs value function iteration for a problem with stochastic transitions.\n",
    "\n",
    "Parameters:\n",
    "- return_function: Function(current_state, shock, next_state) -> reward\n",
    "- state_space: Vector of possible states\n",
    "- shock_space: Vector of possible shocks\n",
    "- transition_probabilities: Matrix of transition probabilities\n",
    "- discount_factor: Discount factor β ∈ (0,1)\n",
    "- tolerance: Convergence tolerance\n",
    "- max_iterations: Maximum number of iterations\n",
    "- initial_value_function: Initial value function\n",
    "\n",
    "Returns:\n",
    "- value_function: Dictionary mapping (state, shock) to values\n",
    "- policy: Dictionary mapping (state, shock) to optimal next states\n",
    "- iterations: Number of iterations until convergence\n",
    "- diffs: Array of iteration-wise maximum values of change in value function\n",
    "\"\"\"\n",
    "function value_iteration_stochastic(\n",
    "    return_function,\n",
    "    state_space,\n",
    "    shock_space,\n",
    "    transition_probabilities,\n",
    "    discount_factor;\n",
    "    tolerance=1e-1,\n",
    "    max_iterations=1000,\n",
    "    initial_value_function=Dict((s, z) => 0.0 for s in state_space for z in shock_space)\n",
    ")\n",
    "    \n",
    "    diffs = Float64[] # track differences\n",
    "\n",
    "    # Initialize value function\n",
    "    value_function = initial_value_function\n",
    "    policy = Dict((s, z) => s for s in state_space for z in shock_space)  # Initial policy: stay at current state\n",
    "    \n",
    "    for iter in 1:max_iterations\n",
    "        max_diff = 0.0\n",
    "        value_function_new = Dict()\n",
    "        \n",
    "        # Update value for each current state and shock\n",
    "        for current_state in state_space\n",
    "            for current_shock in shock_space\n",
    "                # Find expected value over all possible next states\n",
    "                values = Float64[]\n",
    "                \n",
    "                # Try each possible next state\n",
    "                for next_state in state_space\n",
    "                    # Calculate expected value over all possible next shocks\n",
    "                    expected_value = sum(transition_probabilities[current_shock, next_shock] * value_function[(next_state, next_shock)] for next_shock in shock_space)\n",
    "                    \n",
    "                    # Calculate value of choosing this next state:\n",
    "                    # Current reward + discounted expected future value\n",
    "                    value = return_function(current_state, current_shock, next_state) + discount_factor * expected_value\n",
    "                    push!(values, value)\n",
    "                end\n",
    "                \n",
    "                # Update value function and policy\n",
    "                max_value, max_index = findmax(values)\n",
    "                value_function_new[(current_state, current_shock)] = max_value\n",
    "                policy[(current_state, current_shock)] = state_space[max_index]\n",
    "                \n",
    "                # Track maximum change\n",
    "                max_diff = max(max_diff, abs(value_function_new[(current_state, current_shock)] - \n",
    "                                           value_function[(current_state, current_shock)]))\n",
    "            end\n",
    "        end\n",
    "\n",
    "        push!(diffs, max_diff)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if max_diff < tolerance\n",
    "            return value_function_new, policy, iter, diffs\n",
    "        end\n",
    "        \n",
    "        value_function = value_function_new\n",
    "    end\n",
    "    \n",
    "    @warn \"Maximum iterations reached without convergence\"\n",
    "    return value_function, policy, max_iterations, diffs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stoch_rck (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function stoch_rck(k, z, k_next)\n",
    "    # production function: f(k) = Ak^α\n",
    "    α = 0.33\n",
    "    A = 1\n",
    "    δ = 0.1\n",
    "    \n",
    "    # Current production\n",
    "    production = ℯ^z * A*k^α\n",
    "    \n",
    "    # Investment needed to reach k_next\n",
    "    investment = k_next - (1-δ)*k  # δ is depreciation rate\n",
    "    \n",
    "    # Consumption is production minus investment\n",
    "    consumption = production - investment\n",
    "    \n",
    "    # Return negative infinity for infeasible transitions\n",
    "    if consumption <= 0\n",
    "        return -Inf\n",
    "    end\n",
    "    \n",
    "    # Utility from consumption\n",
    "    return log(consumption)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and solve the problem\n",
    "k_min_stoch, k_max_stoch = (0.8 * 3.53287891716), (1.2 * 3.53287891716)\n",
    "state_space_stoch = range(k_min_stoch, k_max_stoch, length=501)\n",
    "discount_factor = 0.96\n",
    "\n",
    "min_shock, max_shock = -0.2, 0.2\n",
    "shock_space = range(min_shock, max_shock, length=201)\n",
    "shock_space = exp.(shock_space)\n",
    "probs = Dict((shock_space[i], shock_space[j]) => 1/201 for i in 1:length(shock_space) for j in 1:length(shock_space))\n",
    "\n",
    "# Solve the problem\n",
    "value_function_stoch, policy_stoch, iterations_stoch, diffs_stoch = value_iteration_stochastic(\n",
    "    stoch_rck,\n",
    "    state_space_stoch,\n",
    "    shock_space,\n",
    "    probs,\n",
    "    discount_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value_iteration_stochastic_new (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function value_iteration_stochastic_new(\n",
    "    return_function,            # Reward function: return_function(current_state, shock, next_state) -> reward\n",
    "    state_space,                # Vector of possible states\n",
    "    shock_space,                # Vector of possible shocks\n",
    "    transition_probabilities,   # Matrix of transition probabilities\n",
    "    discount_factor;            # Discount factor β ∈ (0,1)\n",
    "    tolerance=1e-6,             # Convergence tolerance\n",
    "    max_iterations=1000,        # Maximum number of iterations\n",
    "    initial_value_function=nothing # Initial value function (optional)\n",
    ")\n",
    "    # Map states and shocks to indices\n",
    "    state_to_index = Dict(state_space[i] => i for i in 1:length(state_space))\n",
    "    shock_to_index = Dict(shock_space[j] => j for j in 1:length(shock_space))\n",
    "    \n",
    "    # Initialize value function and policy\n",
    "    S, Z = length(state_space), length(shock_space)\n",
    "    value_function = initial_value_function !== nothing ? initial_value_function : zeros(S, Z)\n",
    "    policy = zeros(Int, S, Z)  # Policy maps to indices of state_space\n",
    "    diffs = Float64[]          # Track differences for convergence\n",
    "\n",
    "    for iter in 1:max_iterations\n",
    "        max_diff = 0.0\n",
    "        value_function_new = copy(value_function)\n",
    "\n",
    "        # Iterate over all states and shocks\n",
    "        for current_state_idx in 1:S\n",
    "            for current_shock_idx in 1:Z\n",
    "                values = zeros(Float64, S)\n",
    "\n",
    "                # Compute value for all possible next states\n",
    "                for next_state_idx in 1:S\n",
    "                    # Expected value over all shocks\n",
    "                    expected_value = sum(\n",
    "                        transition_probabilities[(shock_space[current_shock_idx], shock_space[next_shock_idx])] *\n",
    "                        value_function[next_state_idx, next_shock_idx]\n",
    "                        for next_shock_idx in 1:Z\n",
    "                    )\n",
    "                    # Total value = immediate reward + discounted future value\n",
    "                    values[next_state_idx] = return_function(\n",
    "                        state_space[current_state_idx],\n",
    "                        shock_space[current_shock_idx],\n",
    "                        state_space[next_state_idx]\n",
    "                    ) + discount_factor * expected_value\n",
    "                end\n",
    "\n",
    "                # Find the maximum value and corresponding state\n",
    "                max_value, max_index = findmax(values)\n",
    "                value_function_new[current_state_idx, current_shock_idx] = max_value\n",
    "                policy[current_state_idx, current_shock_idx] = max_index\n",
    "\n",
    "                # Update the maximum difference for convergence check\n",
    "                max_diff = max(max_diff, abs(max_value - value_function[current_state_idx, current_shock_idx]))\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Track the maximum difference for each iteration\n",
    "        push!(diffs, max_diff)\n",
    "        value_function = value_function_new\n",
    "\n",
    "        # Check for convergence\n",
    "        if max_diff < tolerance\n",
    "            return Dict((state_space[i], shock_space[j]) => value_function[i, j] for i in 1:S for j in 1:Z),\n",
    "                   Dict((state_space[i], shock_space[j]) => state_space[policy[i, j]] for i in 1:S for j in 1:Z),\n",
    "                   iter,\n",
    "                   diffs\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Warn if convergence was not achieved within max_iterations\n",
    "    @warn \"Maximum iterations reached without convergence\"\n",
    "    return Dict((state_space[i], shock_space[j]) => value_function[i, j] for i in 1:S for j in 1:Z),\n",
    "           Dict((state_space[i], shock_space[j]) => state_space[policy[i, j]] for i in 1:S for j in 1:Z),\n",
    "           max_iterations,\n",
    "           diffs\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:\n",
      "\n",
      "Stacktrace:\n",
      "  [1] BottomRF\n",
      "    @ ./reduce.jl:86 [inlined]\n",
      "  [2] MappingRF\n",
      "    @ ./reduce.jl:100 [inlined]\n",
      "  [3] _foldl_impl(op::Base.MappingRF{var\"#14#25\"{Vector{Float64}, Dict{Tuple{Float64, Float64}, Float64}, Int64, Int64}, Base.BottomRF{typeof(Base.add_sum)}}, init::Base._InitialValue, itr::UnitRange{Int64})\n",
      "    @ Base ./reduce.jl:62\n",
      "  [4] foldl_impl\n",
      "    @ ./reduce.jl:48 [inlined]\n",
      "  [5] mapfoldl_impl\n",
      "    @ ./reduce.jl:44 [inlined]\n",
      "  [6] mapfoldl\n",
      "    @ ./reduce.jl:175 [inlined]\n",
      "  [7] mapreduce\n",
      "    @ ./reduce.jl:307 [inlined]\n",
      "  [8] sum\n",
      "    @ ./reduce.jl:532 [inlined]\n",
      "  [9] sum\n",
      "    @ ./reduce.jl:561 [inlined]\n",
      " [10] value_iteration_stochastic_new(return_function::typeof(stoch_rck), state_space::StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, shock_space::Vector{Float64}, transition_probabilities::Dict{Tuple{Float64, Float64}, Float64}, discount_factor::Float64; tolerance::Float64, max_iterations::Int64, initial_value_function::Nothing)\n",
      "    @ Main ~/Documents/UChicago/Year_3/macro_core/uchicago_macro_core/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X43sZmlsZQ==.jl:33\n",
      " [11] value_iteration_stochastic_new(return_function::Function, state_space::StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, shock_space::Vector{Float64}, transition_probabilities::Dict{Tuple{Float64, Float64}, Float64}, discount_factor::Float64)\n",
      "    @ Main ~/Documents/UChicago/Year_3/macro_core/uchicago_macro_core/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X43sZmlsZQ==.jl:1\n",
      " [12] top-level scope\n",
      "    @ ~/Documents/UChicago/Year_3/macro_core/uchicago_macro_core/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X44sZmlsZQ==.jl:12"
     ]
    }
   ],
   "source": [
    "# Set up and solve the problem\n",
    "k_min_stoch, k_max_stoch = (0.8 * 3.53287891716), (1.2 * 3.53287891716)\n",
    "state_space_stoch = range(k_min_stoch, k_max_stoch, length=501)\n",
    "discount_factor = 0.96\n",
    "\n",
    "min_shock, max_shock = -0.2, 0.2\n",
    "shock_space = range(min_shock, max_shock, length=201)\n",
    "shock_space = exp.(shock_space)\n",
    "probs = Dict((shock_space[i], shock_space[j]) => 1/201 for i in 1:length(shock_space) for j in 1:length(shock_space))\n",
    "\n",
    "# Solve the problem\n",
    "value_function_stoch, policy_stoch, iterations_stoch, diffs_stoch = value_iteration_stochastic_new(\n",
    "    stoch_rck,\n",
    "    state_space_stoch,\n",
    "    shock_space,\n",
    "    probs,\n",
    "    discount_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of threads: 8\n"
     ]
    }
   ],
   "source": [
    "using Base.Threads\n",
    "\n",
    "# Check the current number of threads\n",
    "println(\"Current number of threads: \", nthreads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value_iteration_stochastic_new_parallel (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function value_iteration_stochastic_new_parallel(\n",
    "    model, state_space, shock_space, probs, discount_factor;\n",
    "    max_iter=1000, tol=1e-6\n",
    ")\n",
    "    V = zeros(length(state_space), length(shock_space))\n",
    "    policy = zeros(length(state_space), length(shock_space))\n",
    "    diff = Inf\n",
    "    iter = 0\n",
    "\n",
    "    while diff > tol && iter < max_iter\n",
    "        V_new = copy(V)\n",
    "        @threads for i in 1:length(state_space)\n",
    "            for j in 1:length(shock_space)\n",
    "                v_max = -Inf\n",
    "                expected_value = 0.0\n",
    "                for k in 1:length(shock_space)\n",
    "                    expected_value += probs[(shock_space[j], shock_space[k])] * V[i, k]\n",
    "                end\n",
    "                value = model(state_space[i], shock_space[j], expected_value) + discount_factor * expected_value\n",
    "                if value > v_max\n",
    "                    v_max = value\n",
    "                    policy[i, j] = shock_space[j]\n",
    "                end\n",
    "                V_new[i, j] = v_max\n",
    "            end\n",
    "        end\n",
    "        diff = maximum(abs.(V_new .- V))\n",
    "        V = V_new\n",
    "        iter += 1\n",
    "    end\n",
    "\n",
    "    return V, policy, iter, diff\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Tuple{Float64, Float64}, Float64} with 100701 entries:\n",
       "  (3.63463, 0.974335) => 0.974335\n",
       "  (3.50179, 1.18057)  => 1.18057\n",
       "  (3.52157, 0.878095) => 0.878095\n",
       "  (3.65158, 1.09856)  => 1.09856\n",
       "  (3.40004, 0.841979) => 0.841979\n",
       "  (3.5781, 0.921272)  => 0.921272\n",
       "  (4.23098, 0.838618) => 0.838618\n",
       "  (3.28416, 0.904837) => 0.904837\n",
       "  (3.16546, 0.921272) => 0.921272\n",
       "  (3.01567, 0.852144) => 0.852144\n",
       "  (3.23046, 1.16416)  => 1.16416\n",
       "  (3.86921, 0.879853) => 0.879853\n",
       "  (4.20554, 0.908464) => 0.908464\n",
       "  (3.40852, 1.08981)  => 1.08981\n",
       "  (3.77594, 0.841979) => 0.841979\n",
       "  (3.2022, 1.17821)   => 1.17821\n",
       "  (3.21916, 1.123)    => 1.123\n",
       "  (3.65158, 0.96464)  => 0.96464\n",
       "  (3.20503, 1.14798)  => 1.14798\n",
       "  ⋮                   => ⋮"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Set up and solve the problem\n",
    "k_min_stoch, k_max_stoch = (0.8 * 3.53287891716), (1.2 * 3.53287891716)\n",
    "state_space_stoch = range(k_min_stoch, k_max_stoch, length=501)\n",
    "discount_factor = 0.96\n",
    "\n",
    "min_shock, max_shock = -0.2, 0.2\n",
    "shock_space = range(min_shock, max_shock, length=201)\n",
    "shock_space = exp.(shock_space)\n",
    "probs = Dict((shock_space[i], shock_space[j]) => 1/201 for i in 1:length(shock_space) for j in 1:length(shock_space))\n",
    "\n",
    "# Solve the problem using the parallel version\n",
    "value_function_stoch, policy_stoch, iterations_stoch, diffs_stoch = value_iteration_stochastic_new_parallel(\n",
    "    stoch_rck,\n",
    "    state_space_stoch,\n",
    "    shock_space,\n",
    "    probs,\n",
    "    discount_factor\n",
    ")\n",
    "\n",
    "# Convert arrays to dictionaries\n",
    "value_function_dict = Dict((state_space_stoch[i], shock_space[j]) => value_function_stoch[i, j] for i in 1:length(state_space_stoch) for j in 1:length(shock_space))\n",
    "policy_function_dict = Dict((state_space_stoch[i], shock_space[j]) => policy_stoch[i, j] for i in 1:length(state_space_stoch) for j in 1:length(shock_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "plotlyjs()\n",
    "\n",
    "# Create a 3D plot\n",
    "x = repeat(state_space_stoch, outer=length(shock_space))\n",
    "y = repeat(shock_space, inner=length(state_space_stoch))\n",
    "z = vec(value_function_stoch)\n",
    "\n",
    "plot(x, y, z, st=:surface, title=\"Value Function for Stochastic RCK Model\", xlabel=\"State\", ylabel=\"Shock\", zlabel=\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.add(\"PlotlyJS\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
